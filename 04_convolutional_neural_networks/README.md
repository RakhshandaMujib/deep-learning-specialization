# Course 4: Convolutional Neural Networks  

**Completed:** 2025

### What I Learned  
- **Convolution & pooling**: How local receptive fields and parameter sharing actually work  
- **CNN architectures**: From LeNet to ResNet, and why deeper isn’t always better  
- **Practical tricks**: Data augmentation, dropout, batch norm, and why they matter  
- **Transfer learning for vision**: Using pre-trained models like VGG/ResNet efficiently  
- **Detection & segmentation basics**: Foundations of object detection, YOLO-style pipelines, and U-Nets  

### My Takeaways  
This is where deep learning stopped being abstract and became *visual and intuitive*.  
I finally understood how models “see”- edges to textures to objects. It also taught me how much performance comes from **architecture choices and data, not just hyperparameters**.  

Honestly, this course made computer vision feel *engineering-real*, not just research-fancy.