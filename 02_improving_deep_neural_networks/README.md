# Course 2: Improving Deep Neural Networks

**Completed:** 2022 (Revisiting in 2025)

### What I Learned
- **Optimization**: Mini-batch gradient descent, Adam, RMSprop, and learning rate scheduling  
- **Regularization**: L2 regularization, dropout, and data augmentation to combat overfitting  
- **Initialization**: He/Xavier initialization to stabilize training  
- **Hyperparameter tuning**: Coarse-to-fine search strategies and why random search often beats grid search  

### My Takeaways
This course taught me how to make models *practical* - understanding that performance is more than just architecture, itâ€™s also about tuning.  
It reshaped how I approach experimentation: **start broad, analyze carefully, and iterate smartly.**
